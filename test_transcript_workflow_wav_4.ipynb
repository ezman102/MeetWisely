{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b770832f",
   "metadata": {},
   "source": [
    "# Smart Meeting Assistant - WAV Transcript Processing\n",
    "This notebook transcribes a meeting from an WAV file, performs speaker diarization, generates a summary, translates the content, and extracts action items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e4edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading DeepSeek model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fd8daa7d954bb595e9e39414df1046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:988: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Oscar\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at 'C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\hyperparams.yaml'\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered parameter transfer hook for _load\n",
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load_if_possible\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain.\n",
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at 'C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\embedding_model.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\embedding_model.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at 'C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\mean_var_norm_emb.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\mean_var_norm_emb.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at 'C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\classifier.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\classifier.ckpt\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at 'C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\label_encoder.ckpt'\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\label_encoder.ckpt\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\embedding_model.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\mean_var_norm_emb.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\classifier.ckpt\n",
      "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\label_encoder.ckpt\n",
      "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from C:\\Users\\Oscar\\.cache\\torch\\pyannote\\speechbrain\\label_encoder.ckpt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\")))\n",
    "\n",
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "from modules.summarizer import generate_summary\n",
    "from modules.translator import translate_text\n",
    "from modules.ds_action_items import extract_action_items_with_deepseek\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load models\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44eb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_diarization(wav_path):\n",
    "    print(f\"üéß Transcribing {wav_path}...\")\n",
    "\n",
    "    result = whisper_model.transcribe(wav_path)\n",
    "    segments = result.get(\"segments\", [])\n",
    "\n",
    "    diarization = diarization_pipeline(wav_path)\n",
    "    speaker_turns = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_turns.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end\n",
    "        })\n",
    "\n",
    "    # Debug print\n",
    "    print(\"\\n--- Whisper Segments ---\")\n",
    "    for seg in segments:\n",
    "        print(f\"Whisper: {seg['start']:.2f}s - {seg['end']:.2f}s ‚Üí {seg['text']}\")\n",
    "\n",
    "    print(\"\\n--- PyAnnote Diarization ---\")\n",
    "    for turn in speaker_turns:\n",
    "        print(f\"PyAnnote: {turn['start']:.2f}s - {turn['end']:.2f}s ‚Üí {turn['speaker']}\")\n",
    "\n",
    "    # Improved speaker matching\n",
    "    speaker_map = {}\n",
    "    speaker_counter = 1\n",
    "    labeled_lines = []\n",
    "\n",
    "    for seg in segments:\n",
    "        start, end = seg['start'], seg['end']\n",
    "        best_match = None\n",
    "        max_overlap = 0.0\n",
    "\n",
    "        for turn in speaker_turns:\n",
    "            overlap_start = max(start, turn[\"start\"])\n",
    "            overlap_end = min(end, turn[\"end\"])\n",
    "            overlap = max(0.0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap > max_overlap:\n",
    "                best_match = turn[\"speaker\"]\n",
    "                max_overlap = overlap\n",
    "\n",
    "        matched_speaker = best_match or \"Unknown\"\n",
    "\n",
    "        if matched_speaker not in speaker_map:\n",
    "            speaker_map[matched_speaker] = f\"Speaker {speaker_counter}\"\n",
    "            speaker_counter += 1\n",
    "\n",
    "        readable_speaker = speaker_map[matched_speaker]\n",
    "        labeled_lines.append(f\"[{readable_speaker}] {seg['text'].strip()}\")\n",
    "\n",
    "    return labeled_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3317444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéß Transcribing assets/sample_meeting_4.wav...\n",
      "\n",
      "--- Whisper Segments ---\n",
      "Whisper: 0.00s - 5.00s ‚Üí  Thanks everyone for joining the quarterly review. Let's start with metrics.\n",
      "Whisper: 5.00s - 11.00s ‚Üí  User retention improved by 12%, churn dropped by 3% in Q2.\n",
      "Whisper: 11.00s - 18.00s ‚Üí  Marketing spent was 10% under budget. Highest ROI from the LinkedIn campaign.\n",
      "Whisper: 18.00s - 23.00s ‚Üí  Engineering velocity increased after switching to two weeks' brints.\n",
      "Whisper: 23.00s - 27.00s ‚Üí  That's promising. Let's replicate that velocity across teams.\n",
      "Whisper: 27.00s - 31.00s ‚Üí  One blocker, test automation is still manual.\n",
      "Whisper: 31.00s - 35.00s ‚Üí  We can pilot playwright tests for the web stack.\n",
      "Whisper: 35.00s - 39.00s ‚Üí  I'll lead the automation pilot with Q18 next week.\n",
      "Whisper: 39.00s - 43.00s ‚Üí  For strategy, we plan to expand into the education sector.\n",
      "Whisper: 43.00s - 49.00s ‚Üí  Agreed. I'll analyze market entry strategies and share by next Monday.\n",
      "Whisper: 49.00s - 52.00s ‚Üí  We'll need a landing page mock-up for that segment.\n",
      "Whisper: 52.00s - 56.00s ‚Üí  I'll coordinate with design to prepare mock-ups.\n",
      "Whisper: 56.00s - 59.00s ‚Üí  Thanks everyone for joining us.\n",
      "\n",
      "--- PyAnnote Diarization ---\n",
      "PyAnnote: 0.03s - 0.06s ‚Üí SPEAKER_00\n",
      "PyAnnote: 0.82s - 4.99s ‚Üí SPEAKER_00\n",
      "PyAnnote: 5.77s - 11.05s ‚Üí SPEAKER_03\n",
      "PyAnnote: 11.05s - 17.33s ‚Üí SPEAKER_02\n",
      "PyAnnote: 18.44s - 22.04s ‚Üí SPEAKER_01\n",
      "PyAnnote: 22.04s - 27.18s ‚Üí SPEAKER_00\n",
      "PyAnnote: 27.96s - 32.03s ‚Üí SPEAKER_02\n",
      "PyAnnote: 32.03s - 34.56s ‚Üí SPEAKER_03\n",
      "PyAnnote: 35.60s - 39.47s ‚Üí SPEAKER_01\n",
      "PyAnnote: 39.47s - 43.03s ‚Üí SPEAKER_00\n",
      "PyAnnote: 43.99s - 48.58s ‚Üí SPEAKER_03\n",
      "PyAnnote: 48.58s - 52.23s ‚Üí SPEAKER_02\n",
      "PyAnnote: 53.22s - 56.48s ‚Üí SPEAKER_01\n",
      "=== Transcript ===\n",
      "[Speaker 1] Thanks everyone for joining the quarterly review. Let's start with metrics.\n",
      "[Speaker 2] User retention improved by 12%, churn dropped by 3% in Q2.\n",
      "[Speaker 3] Marketing spent was 10% under budget. Highest ROI from the LinkedIn campaign.\n",
      "[Speaker 4] Engineering velocity increased after switching to two weeks' brints.\n",
      "[Speaker 1] That's promising. Let's replicate that velocity across teams.\n",
      "[Speaker 3] One blocker, test automation is still manual.\n",
      "[Speaker 2] We can pilot playwright tests for the web stack.\n",
      "[Speaker 4] I'll lead the automation pilot with Q18 next week.\n",
      "[Speaker 1] For strategy, we plan to expand into the education sector.\n",
      "[Speaker 2] Agreed. I'll analyze market entry strategies and share by next Monday.\n",
      "[Speaker 3] We'll need a landing page mock-up for that segment.\n",
      "[Speaker 4] I'll coordinate with design to prepare mock-ups.\n",
      "[Speaker 4] Thanks everyone for joining us.\n"
     ]
    }
   ],
   "source": [
    "# Path to WAV file\n",
    "wav_file = \"assets/sample_meeting_4.wav\"\n",
    "\n",
    "# Transcribe and diarize\n",
    "transcript_lines = transcribe_with_diarization(wav_file)\n",
    "\n",
    "# Show transcript\n",
    "print(\"=== Transcript ===\")\n",
    "print(\"\\n\".join(transcript_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384230d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "In the quarterly review, speakers discuss the performance of the business, strategy and technical matters. They also discuss how to improve the quality of life of users and increase the efficiency of the marketing campaign. They plan to expand into the education sector and prepare a landing page mock-up.\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "summary = generate_summary(\"\\n\".join(transcript_lines))\n",
    "print(\"=== Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26b68928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Translation (French) ===\n",
      "[Speaker 1] Merci √† tous d'avoir particip√© √† l'examen trimestriel. Commen√ßons par les mesures. [Speaker 2] La r√©tention de l'utilisateur s'est am√©lior√©e de 12 %, le churn a chut√© de 3 % au Q2. [Speaker 3] Le marketing a √©t√© d√©pens√© de 10 % sous budget. Le plus haut ROI de la campagne LinkedIn. [Speaker 4] La vitesse de l'ing√©nierie a augment√© apr√®s le passage √† deux semaines de brins. [Speaker 1] C'est prometteur. R√©pliquons cette vitesse entre les √©quipes. [Speaker 3] Un bloqueur, l'automatisation des tests est toujours manuelle. [Speaker 2] Nous pouvons piloter des tests de dramaturge pour la pile web. [Speaker 4] Je dirigerai le pilote d'automatisation avec Q18 la semaine prochaine. [Speaker 1] Pour la strat√©gie, nous pr√©voyons de nous √©tendre au secteur de l'√©ducation. [Speaker 2] Accept√©. Je vais analyser les strat√©gies d'entr√©e sur le march√© et partager d'ici lundi. [Speaker 3] Nous allons rejoindre une maquette de page d'atterrissage pour ce segment. [Speaker 4] Je vais coordonner avec la conception pour pr√©parer des maquettes. [Speakers.\n"
     ]
    }
   ],
   "source": [
    "# Translate to French\n",
    "translation = translate_text(\"\\n\".join(transcript_lines), src_lang=\"en\", tgt_lang=\"fr\")\n",
    "print(\"=== Translation (French) ===\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4b0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Action Items ===\n",
      "Speaker 1:\n",
      "- Task: Ensure teams replicate engineering velocity\n",
      "- Task: Coordinate strategy expansion into education sector\n",
      "\n",
      "Speaker 2:\n",
      "- Task: Analyze market entry strategies for education sector\n",
      "- Task: Share findings by next Monday\n",
      "\n",
      "Speaker 3:\n",
      "- Task: Mention blocker of manual test automation\n",
      "- Task: Coordinate pilot of playwright tests with Q18\n",
      "\n",
      "Speaker 4:\n",
      "- Task: Coordinate with design for education sector landing page mock-ups\n",
      "- Task: Attend next meeting\n",
      "\n",
      "Note: Due dates are not mentioned in the transcript, so they are not included in the action items.\n"
     ]
    }
   ],
   "source": [
    "# Extract Action Items\n",
    "action_items = extract_action_items_with_deepseek(transcript_lines)\n",
    "print(\"=== Action Items ===\")\n",
    "print(action_items)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
