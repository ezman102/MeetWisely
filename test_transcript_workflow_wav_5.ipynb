{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b770832f",
   "metadata": {},
   "source": [
    "# Smart Meeting Assistant - WAV Transcript Processing\n",
    "This notebook transcribes a meeting from an WAV file, performs speaker diarization, generates a summary, translates the content, and extracts action items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e4edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Oscar\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\")))\n",
    "\n",
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "from modules.summarizer import generate_summary\n",
    "from modules.translator import translate_text\n",
    "from modules.ds_action_items import extract_action_items_with_deepseek\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load models\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44eb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_diarization(wav_path):\n",
    "    print(f\"üéß Transcribing {wav_path}...\")\n",
    "\n",
    "    result = whisper_model.transcribe(wav_path)\n",
    "    segments = result.get(\"segments\", [])\n",
    "\n",
    "    diarization = diarization_pipeline(wav_path)\n",
    "    speaker_turns = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_turns.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end\n",
    "        })\n",
    "\n",
    "    # Debug print\n",
    "    print(\"\\n--- Whisper Segments ---\")\n",
    "    for seg in segments:\n",
    "        print(f\"Whisper: {seg['start']:.2f}s - {seg['end']:.2f}s ‚Üí {seg['text']}\")\n",
    "\n",
    "    print(\"\\n--- PyAnnote Diarization ---\")\n",
    "    for turn in speaker_turns:\n",
    "        print(f\"PyAnnote: {turn['start']:.2f}s - {turn['end']:.2f}s ‚Üí {turn['speaker']}\")\n",
    "\n",
    "    # Improved speaker matching\n",
    "    speaker_map = {}\n",
    "    speaker_counter = 1\n",
    "    labeled_lines = []\n",
    "\n",
    "    for seg in segments:\n",
    "        start, end = seg['start'], seg['end']\n",
    "        best_match = None\n",
    "        max_overlap = 0.0\n",
    "\n",
    "        for turn in speaker_turns:\n",
    "            overlap_start = max(start, turn[\"start\"])\n",
    "            overlap_end = min(end, turn[\"end\"])\n",
    "            overlap = max(0.0, overlap_end - overlap_start)\n",
    "\n",
    "            if overlap > max_overlap:\n",
    "                best_match = turn[\"speaker\"]\n",
    "                max_overlap = overlap\n",
    "\n",
    "        matched_speaker = best_match or \"Unknown\"\n",
    "\n",
    "        if matched_speaker not in speaker_map:\n",
    "            speaker_map[matched_speaker] = f\"Speaker {speaker_counter}\"\n",
    "            speaker_counter += 1\n",
    "\n",
    "        readable_speaker = speaker_map[matched_speaker]\n",
    "        labeled_lines.append(f\"[{readable_speaker}] {seg['text'].strip()}\")\n",
    "\n",
    "    return labeled_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3317444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéß Transcribing assets/sample_meeting_3.wav...\n"
     ]
    }
   ],
   "source": [
    "# Path to WAV file\n",
    "wav_file = \"assets/sample_meeting_5.wav\"\n",
    "\n",
    "# Transcribe and diarize\n",
    "transcript_lines = transcribe_with_diarization(wav_file)\n",
    "\n",
    "# Show transcript\n",
    "print(\"=== Transcript ===\")\n",
    "print(\"\\n\".join(transcript_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384230d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "Speaker 1 suggests to include emoji reactions and analytics tracking in the first version of the Q3 release. Speaker 2 proposes to use WebSockets for real-time updates. Speaker 3 proposes to send a draft of the spec by Friday for review.\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "summary = generate_summary(\"\\n\".join(transcript_lines))\n",
    "print(\"=== Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b68928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Translation (French) ===\n",
      "[Speaker 1] Bienvenue √† tout le monde. Finalisons les sp√©cifications pour la version Q3. [Speaker 2] Pour la fonctionnalit√© de chat, je propose d'utiliser WebSockets pour les mises √† jour en temps r√©el. [Speaker 3] Pour la fonctionnalit√© de chat, je propose d'utiliser WebSockets pour les mises √† jour en temps r√©el. [Speaker 1] Je sugg√®re d'inclure les r√©actions emoji dans la premi√®re version. [Speaker 1] Bonne id√©e. Quelqu'un peut-il aussi regarder le suivi analytique? [Speaker 2] Je vais le prendre.\n"
     ]
    }
   ],
   "source": [
    "# Translate to French\n",
    "translation = translate_text(\"\\n\".join(transcript_lines), src_lang=\"en\", tgt_lang=\"fr\")\n",
    "print(\"=== Translation (French) ===\")\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Oscar\\Documents\\GitHub\\MeetWise\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Action Items ===\n",
      "Speaker 1:\n",
      "- Task: Finalize Q3 specs\n",
      "- Task: Discuss emoji reactions\n",
      "- Task: Consider analytics tracking\n",
      "\n",
      "Speaker 2:\n",
      "- Task: Propose WebSockets for chat feature\n",
      "- Task: Implement chat feature using WebSockets\n",
      "- Task: Ensure feature is completed by next sprint\n",
      "\n",
      "Speaker 3:\n",
      "- Task: Review chat feature proposal\n",
      "- Task: Send out draft by Friday\n",
      "- Task: Ensure draft is reviewed by Friday\n"
     ]
    }
   ],
   "source": [
    "# Extract Action Items\n",
    "action_items = extract_action_items_with_deepseek(transcript_lines)\n",
    "print(\"=== Action Items ===\")\n",
    "print(action_items)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
